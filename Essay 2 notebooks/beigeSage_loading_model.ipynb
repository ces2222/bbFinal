{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeigeSage\n",
    "\n",
    "This notebook lets you load and then run the pre-trained and fine-tuned BeigeSage model on texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MCOB PHD 14\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import shutil\n",
    "\n",
    "# Load the sentiment scores CSV\n",
    "excel_path = r\"C:/Users/MCOB PHD 14/Dropbox/Charlie's Dissertation/Beige Books/manual_sentiment.csv\"\n",
    "sentiment_data = pd.read_csv(excel_path)\n",
    "\n",
    "# Define the label function\n",
    "def label_sentiment(score):\n",
    "    if score <= -0.3:\n",
    "        return 0  # Negative\n",
    "    elif score <= 0.2:\n",
    "        return 1  # Mixed\n",
    "    else:\n",
    "        return 2  # Positive\n",
    "\n",
    "# Apply the label function to the sentiment scores\n",
    "sentiment_data['label'] = sentiment_data['human_sentiment'].apply(label_sentiment)\n",
    "\n",
    "# Define path where text files are stored\n",
    "text_files_dir = r\"C:/Users/MCOB PHD 14/Dropbox/Charlie's Dissertation/Beige Books/selected_chunks2\"\n",
    "\n",
    "# Load the text files and create a DataFrame\n",
    "text_data = {}\n",
    "for filename in os.listdir(text_files_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(text_files_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text_data[filename] = file.read()\n",
    "\n",
    "# Combine text data with sentiment data\n",
    "text_df = pd.DataFrame(list(text_data.items()), columns=['file_names', 'text'])\n",
    "combined_data = pd.merge(sentiment_data, text_df, on='file_names')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(combined_data, test_size=0.2, random_state=42, stratify=combined_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228.603\n"
     ]
    }
   ],
   "source": [
    "# average length of text in combined_data\n",
    "print(combined_data['text'].apply(lambda x: len(x.split())).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BeigeSage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Define the path where the model and tokenizer are saved\n",
    "saved_model_path = 'C:/Users/MCOB PHD 14/Desktop/bbFinal/Notebooks/BeigeSage'\n",
    "\n",
    "# Load the saved tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(saved_model_path)\n",
    "\n",
    "# Load the saved model\n",
    "model = RobertaForSequenceClassification.from_pretrained(saved_model_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to tokenize and predict sentiment\n",
    "def predict_sentiment_with_status(texts):\n",
    "    predictions = []\n",
    "    total_texts = len(texts)\n",
    "    for idx, text in enumerate(tqdm(texts, desc=\"Predicting Sentiment\", ncols=100)):\n",
    "        # Tokenize the text\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",       # Return as PyTorch tensors\n",
    "            truncation=True,           # Truncate longer sequences\n",
    "            padding='max_length',      # Pad to max length\n",
    "            max_length=512             # Set maximum length\n",
    "        )\n",
    "        \n",
    "        # Perform prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get the predicted label\n",
    "        predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "        predictions.append(predicted_label)\n",
    "        \n",
    "        # Print status update for every 10 texts\n",
    "        if (idx + 1) % 10 == 0 or (idx + 1) == total_texts:\n",
    "            print(f\"Processed {idx + 1}/{total_texts} texts\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:   5%|██                                      | 10/200 [00:05<01:28,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  10%|████                                    | 20/200 [00:09<01:21,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  15%|██████                                  | 30/200 [00:14<01:15,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  20%|████████                                | 40/200 [00:18<01:11,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  25%|██████████                              | 50/200 [00:23<01:10,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  30%|████████████                            | 60/200 [00:28<01:07,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  35%|██████████████                          | 70/200 [00:32<00:59,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  40%|████████████████                        | 80/200 [00:37<00:56,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  45%|██████████████████                      | 90/200 [00:42<00:51,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  50%|███████████████████▌                   | 100/200 [00:47<00:47,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  55%|█████████████████████▍                 | 110/200 [00:51<00:43,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 110/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  60%|███████████████████████▍               | 120/200 [00:56<00:38,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 120/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  65%|█████████████████████████▎             | 130/200 [01:01<00:33,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 130/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  70%|███████████████████████████▎           | 140/200 [01:06<00:28,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 140/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  75%|█████████████████████████████▎         | 150/200 [01:10<00:24,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 150/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  80%|███████████████████████████████▏       | 160/200 [01:15<00:18,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 160/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  85%|█████████████████████████████████▏     | 170/200 [01:20<00:16,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 170/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  90%|███████████████████████████████████    | 180/200 [01:25<00:09,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 180/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment:  95%|█████████████████████████████████████  | 190/200 [01:30<00:04,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 190/200 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Sentiment: 100%|███████████████████████████████████████| 200/200 [01:35<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200/200 texts\n",
      "                                                  text  predicted_label  \\\n",
      "528  reshaping their job mix . Longer-term , many r...                1   \n",
      "491  December 8 , 1999 The Fifth District economy c...                2   \n",
      "888  over the year in Massachusetts , Boston , and ...                1   \n",
      "899  April 17 , 2019 Summary of Economic Activity S...                2   \n",
      "960  economic conditions visit : https : //www.atla...                1   \n",
      "..                                                 ...              ...   \n",
      "672  , you do n't get the money . '' Looking ahead ...                0   \n",
      "340  split between those expecting the trade balanc...                1   \n",
      "244  activity has been `` unexpectedly quiet '' sin...                1   \n",
      "471  outlook for 1998 , Third District bankers see ...                2   \n",
      "550  for capital investment in the industry , dampe...                1   \n",
      "\n",
      "    predicted_class  \n",
      "528           Mixed  \n",
      "491        Positive  \n",
      "888           Mixed  \n",
      "899        Positive  \n",
      "960           Mixed  \n",
      "..              ...  \n",
      "672        Negative  \n",
      "340           Mixed  \n",
      "244           Mixed  \n",
      "471        Positive  \n",
      "550           Mixed  \n",
      "\n",
      "[200 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Apply the prediction function to the 'text' column of test_data DataFrame\n",
    "test_texts = test_data['text'].tolist()  # Convert text column to list\n",
    "test_data['predicted_label'] = predict_sentiment_with_status(test_texts)\n",
    "\n",
    "# Step 4: Map the numerical labels back to the class names (optional)\n",
    "label_map = {0: \"Negative\", 1: \"Mixed\", 2: \"Positive\"}\n",
    "test_data['predicted_class'] = test_data['predicted_label'].map(label_map)\n",
    "\n",
    "# Display the DataFrame with predictions\n",
    "print(test_data[['text', 'predicted_label', 'predicted_class']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self: Ran model on Sept. 26, 2024, and it took 1 min 35.1 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Mixed       0.67      0.65      0.66        83\n",
      "    Negative       0.69      0.64      0.67        39\n",
      "    Positive       0.77      0.82      0.80        78\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.71      0.70      0.71       200\n",
      "weighted avg       0.71      0.71      0.71       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert label in test_data to be Positive, Negative, or Mixed\n",
    "test_data['label_actual'] = test_data['label'].map({0: \"Negative\", 1: \"Mixed\", 2: \"Positive\"})\n",
    "\n",
    "# Display the classification report\n",
    "print(classification_report(test_data['label_actual'], test_data['predicted_class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>file_names</th>\n",
       "      <th>human_sentiment</th>\n",
       "      <th>scorer</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_actual</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>529</td>\n",
       "      <td>2002_bo (5)_chunk_6.txt</td>\n",
       "      <td>0.2</td>\n",
       "      <td>CS</td>\n",
       "      <td>1</td>\n",
       "      <td>reshaping their job mix . Longer-term , many r...</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>492</td>\n",
       "      <td>1999_ri (1)_chunk_1.txt</td>\n",
       "      <td>0.6</td>\n",
       "      <td>CS</td>\n",
       "      <td>2</td>\n",
       "      <td>December 8 , 1999 The Fifth District economy c...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>2019_bo (6)_chunk_6.txt</td>\n",
       "      <td>0.3</td>\n",
       "      <td>CS</td>\n",
       "      <td>2</td>\n",
       "      <td>over the year in Massachusetts , Boston , and ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>900</td>\n",
       "      <td>2019_ri (6)_chunk_1.txt</td>\n",
       "      <td>0.3</td>\n",
       "      <td>CS</td>\n",
       "      <td>2</td>\n",
       "      <td>April 17 , 2019 Summary of Economic Activity S...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>961</td>\n",
       "      <td>2023_at (7)_chunk_6.txt</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CS</td>\n",
       "      <td>1</td>\n",
       "      <td>economic conditions visit : https : //www.atla...</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Document               file_names  human_sentiment scorer  label  \\\n",
       "528       529  2002_bo (5)_chunk_6.txt              0.2     CS      1   \n",
       "491       492  1999_ri (1)_chunk_1.txt              0.6     CS      2   \n",
       "888       889  2019_bo (6)_chunk_6.txt              0.3     CS      2   \n",
       "899       900  2019_ri (6)_chunk_1.txt              0.3     CS      2   \n",
       "960       961  2023_at (7)_chunk_6.txt              0.0     CS      1   \n",
       "\n",
       "                                                  text label_actual  \\\n",
       "528  reshaping their job mix . Longer-term , many r...        Mixed   \n",
       "491  December 8 , 1999 The Fifth District economy c...     Positive   \n",
       "888  over the year in Massachusetts , Boston , and ...     Positive   \n",
       "899  April 17 , 2019 Summary of Economic Activity S...     Positive   \n",
       "960  economic conditions visit : https : //www.atla...        Mixed   \n",
       "\n",
       "     predicted_label predicted_class  \n",
       "528                1           Mixed  \n",
       "491                2        Positive  \n",
       "888                1           Mixed  \n",
       "899                2        Positive  \n",
       "960                1           Mixed  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename predicted_class as Sentiment_BERT\n",
    "test_data.rename(columns={'predicted_class': 'Sentiment_BeigeSage'}, inplace=True)\n",
    "\n",
    "# Save the test_data DataFrame to a CSV file\n",
    "test_data.to_csv('BeigeSage_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool for testing a particular sentence or paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# Test the model on a sample of text, have it return a label\n",
    "\n",
    "text = \"March 12 , 1975 Recent evidence indicates further weakening in District business activity . Unemployment rose sharply in January , and manufacturers are not so optimistic as they were last November . Nevertheless , bank directors cited several sectors where business activity has improved . In addition , directors report that District businesses have not been confronted with overly excessive inventories ; inflationary pressures have also eased . The District 's rural areas continue to be relatively unaffected by the recession . Unemployment continues to rise . The District 's unemployment rate , seasonally adjusted , increased from 5.9 percent to 6.4 percent between December and January , with the Minneapolis-St. Paul area rate jumping from 5.1 percent to 6.2 percent . In early 1975 , District help-wanted advertising was down 30 percent from a year ago and initial claims for unemployment insurance were up 35 percent . District manufacturers are not so optimistic about their sales outlook as they were last November . After increasing 15.9 percent from a year earlier in the fourth quarter , District manufacturers look for their sales to surpass year-ago levels by around 9 percent during the first nine months of this year . ( Last November respondents had anticipated a 14 percent sales gain during the first half of 1974 . ) Both durables and nondurables producers revised downward their sales expectations , with price increases probably accounting for most , if not all , of the anticipated sales gains . Recent declines in District manufacturing employment also denote\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",       # Return as PyTorch tensors\n",
    "    truncation=True,           # Truncate longer sequences\n",
    "    padding='max_length',      # Pad to max length\n",
    "    max_length=512             # Set maximum length\n",
    ")\n",
    "\n",
    "# Perform prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "predicted_class = label_map[predicted_label]\n",
    "print(f\"Predicted Sentiment: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
