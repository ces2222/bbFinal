{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT with validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and libraries; testing/training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import shutil\n",
    "\n",
    "# Load the sentiment scores CSV\n",
    "excel_path = r\"C:/Users/MCOB PHD 14/Dropbox/Charlie's Dissertation/Beige Books/manual_sentiment.csv\"\n",
    "sentiment_data = pd.read_csv(excel_path)\n",
    "\n",
    "# Define the label function\n",
    "def label_sentiment(score):\n",
    "    if score <= -0.3:\n",
    "        return 0  # Negative\n",
    "    elif score <= 0.2:\n",
    "        return 1  # Mixed\n",
    "    else:\n",
    "        return 2  # Positive\n",
    "\n",
    "# Apply the label function to the sentiment scores\n",
    "sentiment_data['label'] = sentiment_data['human_sentiment'].apply(label_sentiment)\n",
    "\n",
    "# Define path where text files are stored\n",
    "text_files_dir = r\"C:/Users/MCOB PHD 14/Dropbox/Charlie's Dissertation/Beige Books/selected_chunks2\"\n",
    "\n",
    "# Load the text files and create a DataFrame\n",
    "text_data = {}\n",
    "for filename in os.listdir(text_files_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(text_files_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text_data[filename] = file.read()\n",
    "\n",
    "# Combine text data with sentiment data\n",
    "text_df = pd.DataFrame(list(text_data.items()), columns=['file_names', 'text'])\n",
    "combined_data = pd.merge(sentiment_data, text_df, on='file_names')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(combined_data, test_size=0.2, random_state=42, stratify=combined_data['label'])\n",
    "\n",
    "# Save training and testing files in separate folders for future use\n",
    "#train_text_files_dir = r\"C:/Users/MCOB PHD 14/Dropbox/Charlie's Dissertation/Beige Books/train_texts\"\n",
    "#test_text_files_dir = r\"C:/Users/MCOB PHD 14/Dropbox/Charlie's Dissertation/Beige Books/test_texts\"\n",
    "#os.makedirs(train_text_files_dir, exist_ok=True)\n",
    "#os.makedirs(test_text_files_dir, exist_ok=True)\n",
    "\n",
    "for _, row in train_data.iterrows():\n",
    "    src_path = os.path.join(text_files_dir, row['file_names'])\n",
    "    dest_path = os.path.join(train_text_files_dir, row['file_names'])\n",
    "    shutil.copyfile(src_path, dest_path)\n",
    "\n",
    "for _, row in test_data.iterrows():\n",
    "    src_path = os.path.join(text_files_dir, row['file_names'])\n",
    "    dest_path = os.path.join(test_text_files_dir, row['file_names'])\n",
    "    shutil.copyfile(src_path, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MCOB PHD 14\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_texts(texts, max_length=512):\n",
    "    tokenized_texts = []\n",
    "    for text in texts:\n",
    "        tokenized_input = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',   # Pad to max_length\n",
    "            truncation=True,        # Truncate if longer than max_length\n",
    "            max_length=max_length,  # Maximum length for input\n",
    "            return_tensors='pt'     # Return PyTorch tensors\n",
    "        )\n",
    "        tokenized_texts.append({\n",
    "            'input_ids': tokenized_input['input_ids'].squeeze(),\n",
    "            'attention_mask': tokenized_input['attention_mask'].squeeze()\n",
    "        })\n",
    "    return tokenized_texts\n",
    "\n",
    "# Tokenize training and testing texts\n",
    "train_texts = train_data['text'].tolist()\n",
    "train_labels = train_data['label'].tolist()\n",
    "tokenized_train_texts = tokenize_texts(train_texts)\n",
    "\n",
    "test_texts = test_data['text'].tolist()\n",
    "test_labels = test_data['label'].tolist()\n",
    "tokenized_test_texts = tokenize_texts(test_texts)\n",
    "\n",
    "# Create custom PyTorch Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenized_texts[idx]['input_ids']\n",
    "        attention_mask = self.tokenized_texts[idx]['attention_mask']\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': label}\n",
    "\n",
    "# Create the training and testing datasets\n",
    "train_dataset = SentimentDataset(tokenized_train_texts, train_labels)\n",
    "test_dataset = SentimentDataset(tokenized_test_texts, test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                 \n",
      " 33%|███▎      | 100/300 [20:42<38:50, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0229394435882568, 'eval_runtime': 83.7562, 'eval_samples_per_second': 2.388, 'eval_steps_per_second': 0.298, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 200/300 [40:41<19:16, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.783829927444458, 'eval_runtime': 84.6447, 'eval_samples_per_second': 2.363, 'eval_steps_per_second': 0.295, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 300/300 [1:01:28<00:00, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8426475524902344, 'eval_runtime': 84.1755, 'eval_samples_per_second': 2.376, 'eval_steps_per_second': 0.297, 'epoch': 3.0}\n",
      "{'train_runtime': 3688.6682, 'train_samples_per_second': 0.651, 'train_steps_per_second': 0.081, 'train_loss': 0.953960673014323, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.953960673014323, metrics={'train_runtime': 3688.6682, 'train_samples_per_second': 0.651, 'train_steps_per_second': 0.081, 'total_flos': 631472202547200.0, 'train_loss': 0.953960673014323, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the BERT model with a classification head\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    eval_strategy=\"epoch\",           # Evaluate at the end of each epoch\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with the datasets and model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model on the training dataset\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:22<00:00,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.57      0.87      0.69        39\n",
      "       Mixed       0.56      0.73      0.64        83\n",
      "    Positive       0.97      0.38      0.55        78\n",
      "\n",
      "    accuracy                           0.62       200\n",
      "   macro avg       0.70      0.66      0.62       200\n",
      "weighted avg       0.72      0.62      0.61       200\n",
      "\n",
      "Accuracy on test data: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "predictions, true_labels, _ = trainer.predict(test_dataset)\n",
    "\n",
    "# Convert predictions to label indices\n",
    "predicted_labels = torch.argmax(torch.tensor(predictions), axis=1)\n",
    "\n",
    "# Print a classification report for the test dataset\n",
    "report = classification_report(true_labels, predicted_labels, target_names=[\"Negative\", \"Mixed\", \"Positive\"])\n",
    "print(report)\n",
    "\n",
    "# Calculate and print the accuracy on the test dataset\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f'Accuracy on test data: {accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./BeigeBERT_three_validated\\\\tokenizer_config.json',\n",
       " './BeigeBERT_three_validated\\\\special_tokens_map.json',\n",
       " './BeigeBERT_three_validated\\\\vocab.txt',\n",
       " './BeigeBERT_three_validated\\\\added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "\n",
    "model.save_pretrained('./BeigeBERT_three_validated')\n",
    "tokenizer.save_pretrained('./BeigeBERT_three_validated')\n",
    "\n",
    "# The model and tokenizer can be loaded later for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MCOB PHD 14\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the RoBERTa tokenizer and re-tokenize the text data\n",
    "\n",
    "# Load the RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Function to tokenize text using RoBERTa tokenizer\n",
    "def tokenize_texts_roberta(texts, max_length=512):\n",
    "    tokenized_texts = []\n",
    "    for text in texts:\n",
    "        tokenized_input = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',    # Pad all sequences to max_length\n",
    "            truncation=True,         # Truncate sequences longer than max_length\n",
    "            max_length=max_length,   # Set maximum length for sequences\n",
    "            return_tensors='pt'      # Return PyTorch tensors\n",
    "        )\n",
    "        tokenized_texts.append({\n",
    "            'input_ids': tokenized_input['input_ids'].squeeze(), \n",
    "            'attention_mask': tokenized_input['attention_mask'].squeeze()\n",
    "        })\n",
    "    return tokenized_texts\n",
    "\n",
    "# Re-tokenize training and testing texts\n",
    "tokenized_train_texts_roberta = tokenize_texts_roberta(train_texts)\n",
    "tokenized_test_texts_roberta = tokenize_texts_roberta(test_texts)\n",
    "\n",
    "# Step 2: Create PyTorch datasets using re-tokenized data\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenized_texts[idx]['input_ids']\n",
    "        attention_mask = self.tokenized_texts[idx]['attention_mask']\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': label}\n",
    "\n",
    "# Create the datasets using re-tokenized data\n",
    "train_dataset_roberta = SentimentDataset(tokenized_train_texts_roberta, train_labels)\n",
    "test_dataset_roberta = SentimentDataset(tokenized_test_texts_roberta, test_labels)\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory for saving model checkpoints\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    eval_strategy=\"epoch\",           # Evaluate at the end of each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                 \n",
      " 33%|███▎      | 100/300 [21:04<39:29, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0409733057022095, 'eval_runtime': 95.0076, 'eval_samples_per_second': 2.105, 'eval_steps_per_second': 0.263, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 200/300 [42:33<20:00, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7915773987770081, 'eval_runtime': 94.8621, 'eval_samples_per_second': 2.108, 'eval_steps_per_second': 0.264, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 300/300 [1:04:11<00:00, 12.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6947354674339294, 'eval_runtime': 92.7937, 'eval_samples_per_second': 2.155, 'eval_steps_per_second': 0.269, 'epoch': 3.0}\n",
      "{'train_runtime': 3851.31, 'train_samples_per_second': 0.623, 'train_steps_per_second': 0.078, 'train_loss': 0.9571084594726562, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.9571084594726562, metrics={'train_runtime': 3851.31, 'train_samples_per_second': 0.623, 'train_steps_per_second': 0.078, 'total_flos': 631472202547200.0, 'train_loss': 0.9571084594726562, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 4: Initialize the Trainer with the re-tokenized datasets and RoBERTa model\n",
    "\n",
    "# Load the RoBERTa model with a classification head\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_roberta,   # Use the re-tokenized training dataset\n",
    "    eval_dataset=test_dataset_roberta,     # Use the re-tokenized testing dataset\n",
    "    # Optionally pass class weights to the Trainer\n",
    "    # compute_metrics=...  # Add custom evaluation metrics if needed\n",
    ")\n",
    "\n",
    "# Step 5: Fine-tune the model on the re-tokenized training dataset\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:31<00:00,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.63      0.79      0.70        39\n",
      "       Mixed       0.64      0.65      0.65        83\n",
      "    Positive       0.79      0.68      0.73        78\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.69      0.71      0.69       200\n",
      "weighted avg       0.70      0.69      0.69       200\n",
      "\n",
      "RoBERTa accuracy on test data: 0.6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate the model on the re-tokenized test dataset\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "roberta_predictions, roberta_true_labels, _ = trainer.predict(test_dataset_roberta)\n",
    "\n",
    "# Convert predictions to label indices\n",
    "roberta_predicted_labels = torch.argmax(torch.tensor(roberta_predictions), axis=1)\n",
    "\n",
    "# Print a classification report for the test dataset\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "roberta_report = classification_report(roberta_true_labels, roberta_predicted_labels, target_names=[\"Negative\", \"Mixed\", \"Positive\"])\n",
    "print(roberta_report)\n",
    "\n",
    "# Calculate and print the accuracy on the test dataset\n",
    "roberta_accuracy = accuracy_score(roberta_true_labels, roberta_predicted_labels)\n",
    "print(f'RoBERTa accuracy on test data: {roberta_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./RoBERTa_three_validated\\\\tokenizer_config.json',\n",
       " './RoBERTa_three_validated\\\\special_tokens_map.json',\n",
       " './RoBERTa_three_validated\\\\vocab.json',\n",
       " './RoBERTa_three_validated\\\\merges.txt',\n",
       " './RoBERTa_three_validated\\\\added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the RoBERTa model and tokenizer\n",
    "model.save_pretrained('./RoBERTa_three_validated')\n",
    "tokenizer.save_pretrained('./RoBERTa_three_validated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER test\n",
    "\n",
    "We're going to:\n",
    "\n",
    "1. Calculate sentiment on the training texts using VADER.\n",
    "2. Change those to class labels based on the same thresholds.\n",
    "3. Calculate the classification error rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER accuracy on training data: 0.4950\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.52      0.44      0.47        39\n",
      "       Mixed       0.53      0.10      0.16        83\n",
      "    Positive       0.49      0.95      0.64        78\n",
      "\n",
      "    accuracy                           0.49       200\n",
      "   macro avg       0.51      0.49      0.43       200\n",
      "weighted avg       0.51      0.49      0.41       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[17  4 18]\n",
      " [15  8 60]\n",
      " [ 1  3 74]]\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Step 1: Calculate Sentiment Scores Using VADER\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate VADER sentiment scores for training texts\n",
    "vader_scores = [analyzer.polarity_scores(text)['compound'] for text in test_texts]\n",
    "\n",
    "# Step 2: Convert Sentiment Scores to Class Labels\n",
    "\n",
    "# Define the same label conversion function used for the model\n",
    "def vader_to_label(score):\n",
    "    if score < -0.2:\n",
    "        return 0  # Negative\n",
    "    elif score <= 0.2:\n",
    "        return 1  # Mixed\n",
    "    else:\n",
    "        return 2  # Positive\n",
    "\n",
    "# Convert VADER scores to class labels\n",
    "vader_labels = [vader_to_label(score) for score in vader_scores]\n",
    "\n",
    "# Step 3: Calculate Classification Error Rates\n",
    "\n",
    "# Calculate accuracy\n",
    "vader_accuracy = accuracy_score(test_labels, vader_labels)\n",
    "print(f'VADER accuracy on training data: {vader_accuracy:.4f}')\n",
    "\n",
    "# Calculate classification report\n",
    "vader_classification_report = classification_report(test_labels, vader_labels, target_names=[\"Negative\", \"Mixed\", \"Positive\"])\n",
    "print(\"Classification Report:\\n\", vader_classification_report)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "vader_confusion_matrix = confusion_matrix(test_labels, vader_labels)\n",
    "print(\"Confusion Matrix:\\n\", vader_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.52      0.44      0.47        39\n",
      "       Mixed       0.53      0.10      0.16        83\n",
      "    Positive       0.49      0.95      0.64        78\n",
      "\n",
      "    accuracy                           0.49       200\n",
      "   macro avg       0.51      0.49      0.43       200\n",
      "weighted avg       0.51      0.49      0.41       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(vader_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert vader_labels to positive, negative, and mixed\n",
    "def vader_to_label(score):\n",
    "    if score < -0.2:\n",
    "        return 'negative'\n",
    "    elif score <= 0.2:\n",
    "        return 'mixed'\n",
    "    else:\n",
    "        return 'positive'\n",
    "    \n",
    "vader_labels = [vader_to_label(score) for score in vader_scores]\n",
    "\n",
    "# Save to a dataframe\n",
    "vader_df = pd.DataFrame({'file_names': test_data['file_names'], 'vader_sentiment': vader_labels})\n",
    "\n",
    "vader_df.head()\n",
    "\n",
    "# Save to a CSV\n",
    "vader_df.to_csv('vader_sentiment.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
