{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize OpenAI\n",
    "def initialize_openai():\n",
    "    OpenAI.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    return openai\n",
    "\n",
    "# Function to get OpenAI instructions\n",
    "def get_openai_instructions():\n",
    "    return \"Label the overall economic sentiment in the following Beige Book text as negative, mixed, or positive. Return the label as a JSON value with a key of 'label' and nothing else:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate OpenAI completion\n",
    "def generate_openai_completion(client, sentence, instructions):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": sentence}\n",
    "        ],\n",
    "        seed=1985,\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "    return completion.choices[0].message\n",
    "\n",
    "# Function to process a single sentence\n",
    "def process_sentence(sentence):\n",
    "    client = initialize_openai()\n",
    "    instructions = get_openai_instructions()\n",
    "    return generate_openai_completion(client, sentence, instructions)\n",
    "\n",
    "# Function to process multiple sentences\n",
    "def process_sentences(sentences):\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(process_sentence, sentences))\n",
    "    return results\n",
    "\n",
    "# Function to calculate means for each topic\n",
    "def calculate_topic_means(sentiment_scores):\n",
    "    topic_means = {}\n",
    "    for topic, scores in sentiment_scores.items():\n",
    "        non_none_scores = [score for score in scores if score is not None]\n",
    "        if non_none_scores:\n",
    "            topic_means[topic] = np.mean(non_none_scores)\n",
    "        else:\n",
    "            topic_means[topic] = None\n",
    "    return topic_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader # For loading text documents for use in LLMs\n",
    "from langchain_community.document_loaders import DirectoryLoader # For loading directories of text documents for use in LLMs\n",
    "import magic # This is needed for some reason for the DirectoryLoader to work\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Do that weird \"r\" thing because paths are weird in Windows\n",
    "path = r\"C:\\Users\\MCOB PHD 14\\Dropbox\\Charlie's Dissertation\\Beige Books\\test_texts\"\n",
    "\n",
    "# Give it the path and tell it to look for .txt files\n",
    "loader = DirectoryLoader(path, glob=\"**/*.txt\")\n",
    "\n",
    "# Read in all the documents from that directory as a list\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Beige Book: 1970_ch (1)_chunk_4.txt\n",
      "Processed Beige Book: 1970_cl (6)_chunk_1.txt\n",
      "Processed Beige Book: 1970_ny (1)_chunk_3.txt\n",
      "Processed Beige Book: 1971_kc (12)_chunk_2.txt\n",
      "Processed Beige Book: 1972_at (9)_chunk_2.txt\n",
      "Processed Beige Book: 1972_kc (5)_chunk_1.txt\n",
      "Processed Beige Book: 1972_ny (10)_chunk_2.txt\n",
      "Processed Beige Book: 1973_cl (10)_chunk_2.txt\n",
      "Processed Beige Book: 1973_cl (12)_chunk_3.txt\n",
      "Processed Beige Book: 1973_cl (2)_chunk_3.txt\n",
      "Processed Beige Book: 1973_sl (5)_chunk_4.txt\n",
      "Processed Beige Book: 1974_at (1)_chunk_5.txt\n",
      "Processed Beige Book: 1975_at (8)_chunk_4.txt\n",
      "Processed Beige Book: 1975_bo (5)_chunk_3.txt\n",
      "Processed Beige Book: 1976_da (9)_chunk_2.txt\n",
      "Processed Beige Book: 1976_ns (7)_chunk_4.txt\n",
      "Processed Beige Book: 1976_ny (6)_chunk_3.txt\n",
      "Processed Beige Book: 1976_ph (7)_chunk_1.txt\n",
      "Processed Beige Book: 1976_ri (4)_chunk_1.txt\n",
      "Processed Beige Book: 1976_sf (3)_chunk_4.txt\n",
      "Processed Beige Book: 1977_ch (12)_chunk_3.txt\n",
      "Processed Beige Book: 1977_cl (6)_chunk_1.txt\n",
      "Processed Beige Book: 1977_kc (3)_chunk_3.txt\n",
      "Processed Beige Book: 1977_mn (12)_chunk_4.txt\n",
      "Processed Beige Book: 1977_ph (4)_chunk_1.txt\n",
      "Processed Beige Book: 1978_at (6)_chunk_1.txt\n",
      "Processed Beige Book: 1978_sl (8)_chunk_3.txt\n",
      "Processed Beige Book: 1979_bo (8)_chunk_2.txt\n",
      "Processed Beige Book: 1979_ch (1)_chunk_4.txt\n",
      "Processed Beige Book: 1979_sf (9)_chunk_1.txt\n",
      "Processed Beige Book: 1979_sf (9)_chunk_4.txt\n",
      "Processed Beige Book: 1980_at (6)_chunk_4.txt\n",
      "Processed Beige Book: 1980_ri (9)_chunk_1.txt\n",
      "Processed Beige Book: 1981_cl (6)_chunk_2.txt\n",
      "Processed Beige Book: 1981_ns (4)_chunk_5.txt\n",
      "Processed Beige Book: 1982_ch (8)_chunk_5.txt\n",
      "Processed Beige Book: 1983_ph (3)_chunk_4.txt\n",
      "Processed Beige Book: 1983_sl (3)_chunk_3.txt\n",
      "Processed Beige Book: 1984_ch (1)_chunk_2.txt\n",
      "Processed Beige Book: 1985_ns (1)_chunk_1.txt\n",
      "Processed Beige Book: 1986_at (3)_chunk_4.txt\n",
      "Processed Beige Book: 1987_bo (5)_chunk_2.txt\n",
      "Processed Beige Book: 1988_sl (7)_chunk_3.txt\n",
      "Processed Beige Book: 1989_cl (6)_chunk_1.txt\n",
      "Processed Beige Book: 1989_da (2)_chunk_4.txt\n",
      "Processed Beige Book: 1989_da (3)_chunk_2.txt\n",
      "Processed Beige Book: 1989_kc (4)_chunk_3.txt\n",
      "Processed Beige Book: 1989_mn (7)_chunk_2.txt\n",
      "Processed Beige Book: 1990_ny (1)_chunk_3.txt\n",
      "Processed Beige Book: 1990_ri (1)_chunk_2.txt\n",
      "Processed Beige Book: 1990_sf (1)_chunk_1.txt\n",
      "Processed Beige Book: 1990_sf (1)_chunk_2.txt\n",
      "Processed Beige Book: 1990_sl (5)_chunk_2.txt\n",
      "Processed Beige Book: 1991_ns (6)_chunk_1.txt\n",
      "Processed Beige Book: 1991_ri (8)_chunk_2.txt\n",
      "Processed Beige Book: 1991_sf (8)_chunk_4.txt\n",
      "Processed Beige Book: 1992_at (7)_chunk_2.txt\n",
      "Processed Beige Book: 1992_cl (6)_chunk_5.txt\n",
      "Processed Beige Book: 1992_cl (8)_chunk_4.txt\n",
      "Processed Beige Book: 1992_mn (4)_chunk_2.txt\n",
      "Processed Beige Book: 1992_mn (8)_chunk_3.txt\n",
      "Processed Beige Book: 1992_ny (5)_chunk_2.txt\n",
      "Processed Beige Book: 1992_ph (4)_chunk_4.txt\n",
      "Processed Beige Book: 1992_ph (6)_chunk_1.txt\n",
      "Processed Beige Book: 1992_sl (4)_chunk_1.txt\n",
      "Processed Beige Book: 1993_bo (1)_chunk_2.txt\n",
      "Processed Beige Book: 1993_ch (4)_chunk_3.txt\n",
      "Processed Beige Book: 1993_ns (1)_chunk_5.txt\n",
      "Processed Beige Book: 1993_sl (5)_chunk_3.txt\n",
      "Processed Beige Book: 1993_sl (6)_chunk_3.txt\n",
      "Processed Beige Book: 1994_at (3)_chunk_3.txt\n",
      "Processed Beige Book: 1994_bo (5)_chunk_4.txt\n",
      "Processed Beige Book: 1994_bo (7)_chunk_1.txt\n",
      "Processed Beige Book: 1994_ch (6)_chunk_1.txt\n",
      "Processed Beige Book: 1994_ch (7)_chunk_4.txt\n",
      "Processed Beige Book: 1994_mn (7)_chunk_5.txt\n",
      "Processed Beige Book: 1994_ph (7)_chunk_5.txt\n",
      "Processed Beige Book: 1995_ch (4)_chunk_2.txt\n",
      "Processed Beige Book: 1995_cl (2)_chunk_3.txt\n",
      "Processed Beige Book: 1995_cl (6)_chunk_3.txt\n",
      "Processed Beige Book: 1995_ns (1)_chunk_2.txt\n",
      "Processed Beige Book: 1995_ns (4)_chunk_7.txt\n",
      "Processed Beige Book: 1996_da (1)_chunk_5.txt\n",
      "Processed Beige Book: 1996_mn (3)_chunk_1.txt\n",
      "Processed Beige Book: 1997_bo (6)_chunk_3.txt\n",
      "Processed Beige Book: 1997_ny (2)_chunk_3.txt\n",
      "Processed Beige Book: 1997_ph (2)_chunk_4.txt\n",
      "Processed Beige Book: 1997_sf (1)_chunk_1.txt\n",
      "Processed Beige Book: 1998_da (4)_chunk_4.txt\n",
      "Processed Beige Book: 1998_kc (3)_chunk_2.txt\n",
      "Processed Beige Book: 1998_ph (8)_chunk_4.txt\n",
      "Processed Beige Book: 1999_ch (5)_chunk_2.txt\n",
      "Processed Beige Book: 1999_ch (8)_chunk_2.txt\n",
      "Processed Beige Book: 1999_cl (3)_chunk_3.txt\n",
      "Processed Beige Book: 1999_ny (8)_chunk_1.txt\n",
      "Processed Beige Book: 1999_ph (8)_chunk_2.txt\n",
      "Processed Beige Book: 1999_ri (1)_chunk_1.txt\n",
      "Processed Beige Book: 2000_at (2)_chunk_2.txt\n",
      "Processed Beige Book: 2000_kc (2)_chunk_1.txt\n",
      "Processed Beige Book: 2001_ns (1)_chunk_1.txt\n",
      "Processed Beige Book: 2001_ny (8)_chunk_5.txt\n",
      "Processed Beige Book: 2001_ri (5)_chunk_4.txt\n",
      "Processed Beige Book: 2002_bo (5)_chunk_6.txt\n",
      "Processed Beige Book: 2003_bo (4)_chunk_4.txt\n",
      "Processed Beige Book: 2003_ch (7)_chunk_1.txt\n",
      "Processed Beige Book: 2003_cl (3)_chunk_2.txt\n",
      "Processed Beige Book: 2003_cl (7)_chunk_6.txt\n",
      "Processed Beige Book: 2003_da (7)_chunk_4.txt\n",
      "Processed Beige Book: 2003_ns (1)_chunk_9.txt\n",
      "Processed Beige Book: 2003_ny (2)_chunk_4.txt\n",
      "Processed Beige Book: 2003_ri (6)_chunk_4.txt\n",
      "Processed Beige Book: 2003_ri (7)_chunk_1.txt\n",
      "Processed Beige Book: 2004_bo (7)_chunk_1.txt\n",
      "Processed Beige Book: 2004_kc (3)_chunk_1.txt\n",
      "Processed Beige Book: 2004_kc (4)_chunk_2.txt\n",
      "Processed Beige Book: 2004_kc (6)_chunk_2.txt\n",
      "Processed Beige Book: 2004_mn (8)_chunk_1.txt\n",
      "Processed Beige Book: 2005_kc (1)_chunk_4.txt\n",
      "Processed Beige Book: 2005_ri (5)_chunk_6.txt\n",
      "Processed Beige Book: 2006_ch (6)_chunk_3.txt\n",
      "Processed Beige Book: 2006_ns (7)_chunk_4.txt\n",
      "Processed Beige Book: 2006_ns (8)_chunk_8.txt\n",
      "Processed Beige Book: 2006_ri (1)_chunk_3.txt\n",
      "Processed Beige Book: 2007_bo (7)_chunk_4.txt\n",
      "Processed Beige Book: 2007_da (6)_chunk_3.txt\n",
      "Processed Beige Book: 2007_ns (6)_chunk_7.txt\n",
      "Processed Beige Book: 2008_ch (1)_chunk_5.txt\n",
      "Processed Beige Book: 2008_cl (3)_chunk_1.txt\n",
      "Processed Beige Book: 2008_cl (8)_chunk_3.txt\n",
      "Processed Beige Book: 2008_da (4)_chunk_6.txt\n",
      "Processed Beige Book: 2008_mn (5)_chunk_5.txt\n",
      "Processed Beige Book: 2008_ph (3)_chunk_4.txt\n",
      "Processed Beige Book: 2008_sf (2)_chunk_3.txt\n",
      "Processed Beige Book: 2009_ch (8)_chunk_5.txt\n",
      "Processed Beige Book: 2009_mn (6)_chunk_3.txt\n",
      "Processed Beige Book: 2010_ns (4)_chunk_7.txt\n",
      "Processed Beige Book: 2011_kc (3)_chunk_5.txt\n",
      "Processed Beige Book: 2011_ny (8)_chunk_5.txt\n",
      "Processed Beige Book: 2011_sf (7)_chunk_2.txt\n",
      "Processed Beige Book: 2012_bo (1)_chunk_3.txt\n",
      "Processed Beige Book: 2012_cl (2)_chunk_7.txt\n",
      "Processed Beige Book: 2012_kc (1)_chunk_1.txt\n",
      "Processed Beige Book: 2012_ny (7)_chunk_5.txt\n",
      "Processed Beige Book: 2013_ny (4)_chunk_2.txt\n",
      "Processed Beige Book: 2013_ri (8)_chunk_6.txt\n",
      "Processed Beige Book: 2013_sf (5)_chunk_1.txt\n",
      "Processed Beige Book: 2014_at (8)_chunk_4.txt\n",
      "Processed Beige Book: 2014_ny (1)_chunk_4.txt\n",
      "Processed Beige Book: 2014_ph (2)_chunk_3.txt\n",
      "Processed Beige Book: 2014_ph (8)_chunk_4.txt\n",
      "Processed Beige Book: 2015_ch (3)_chunk_3.txt\n",
      "Processed Beige Book: 2015_cl (8)_chunk_1.txt\n",
      "Processed Beige Book: 2015_ns (6)_chunk_4.txt\n",
      "Processed Beige Book: 2015_ns (6)_chunk_8.txt\n",
      "Processed Beige Book: 2016_bo (6)_chunk_3.txt\n",
      "Processed Beige Book: 2016_kc (2)_chunk_1.txt\n",
      "Processed Beige Book: 2016_ns (4)_chunk_1.txt\n",
      "Processed Beige Book: 2016_ns (6)_chunk_3.txt\n",
      "Processed Beige Book: 2017_at (3)_chunk_2.txt\n",
      "Processed Beige Book: 2017_bo (5)_chunk_1.txt\n",
      "Processed Beige Book: 2017_kc (8)_chunk_5.txt\n",
      "Processed Beige Book: 2017_sl (1)_chunk_1.txt\n",
      "Processed Beige Book: 2017_sl (5)_chunk_6.txt\n",
      "Processed Beige Book: 2018_at (7)_chunk_4.txt\n",
      "Processed Beige Book: 2018_bo (7)_chunk_4.txt\n",
      "Processed Beige Book: 2018_ns (6)_chunk_3.txt\n",
      "Processed Beige Book: 2018_sl (8)_chunk_1.txt\n",
      "Processed Beige Book: 2019_bo (6)_chunk_6.txt\n",
      "Processed Beige Book: 2019_ny (2)_chunk_4.txt\n",
      "Processed Beige Book: 2019_ph (4)_chunk_6.txt\n",
      "Processed Beige Book: 2019_ph (6)_chunk_1.txt\n",
      "Processed Beige Book: 2019_ri (6)_chunk_1.txt\n",
      "Processed Beige Book: 2019_ri (8)_chunk_4.txt\n",
      "Processed Beige Book: 2020_at (1)_chunk_2.txt\n",
      "Processed Beige Book: 2020_at (4)_chunk_6.txt\n",
      "Processed Beige Book: 2020_kc (1)_chunk_2.txt\n",
      "Processed Beige Book: 2020_mn (2)_chunk_6.txt\n",
      "Processed Beige Book: 2021_ch (7)_chunk_4.txt\n",
      "Processed Beige Book: 2021_cl (1)_chunk_3.txt\n",
      "Processed Beige Book: 2021_cl (7)_chunk_1.txt\n",
      "Processed Beige Book: 2022_mn (5)_chunk_5.txt\n",
      "Processed Beige Book: 2022_mn (6)_chunk_6.txt\n",
      "Processed Beige Book: 2022_ny (2)_chunk_6.txt\n",
      "Processed Beige Book: 2022_ph (4)_chunk_5.txt\n",
      "Processed Beige Book: 2022_ph (8)_chunk_4.txt\n",
      "Processed Beige Book: 2022_ri (1)_chunk_1.txt\n",
      "Processed Beige Book: 2022_ri (7)_chunk_1.txt\n",
      "Processed Beige Book: 2023_at (7)_chunk_6.txt\n",
      "Processed Beige Book: 2023_at (8)_chunk_4.txt\n",
      "Processed Beige Book: 2023_kc (7)_chunk_2.txt\n",
      "Processed Beige Book: 2023_mn (3)_chunk_6.txt\n",
      "Processed Beige Book: 2023_ns (7)_chunk_2.txt\n",
      "Processed Beige Book: 2023_ns (8)_chunk_6.txt\n",
      "Processed Beige Book: 2023_ph (1)_chunk_6.txt\n",
      "Processed Beige Book: 2023_ph (6)_chunk_5.txt\n",
      "Processed Beige Book: 2024_at (6)_chunk_3.txt\n",
      "Processed Beige Book: 2024_da (5)_chunk_1.txt\n",
      "Processed Beige Book: 2024_da (5)_chunk_6.txt\n",
      "Processed Beige Book: 2024_da (8)_chunk_3.txt\n",
      "Processed Beige Book: 2024_ri (8)_chunk_5.txt\n",
      "All documents processed and saved to 'dfOpenAI_labels.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "# Placeholder function to process a document and return a single label.\n",
    "def process_document(document_content):\n",
    "    # Here, process the whole document content and return a sentiment label.\n",
    "    # For example, if you have a model or an API call, it can be used here.\n",
    "    # Assuming the function returns 'positive', 'negative', or 'neutral'.\n",
    "    document_label = process_sentence(document_content)  # Replace with actual function call\n",
    "    return document_label\n",
    "\n",
    "# Initialize list to store results\n",
    "document_results = []\n",
    "\n",
    "# Process each document in `docs`\n",
    "for document in docs:\n",
    "    # Extract Beige Book name and content from the document\n",
    "    beige_book = document.metadata['source'].split(\"\\\\\")[-1]  # Extract file name from path\n",
    "    page_content = document.page_content\n",
    "\n",
    "    # Process the entire document to get a single sentiment label\n",
    "    document_label = process_document(page_content)\n",
    "    \n",
    "    # Append document-level result\n",
    "    document_results.append({\n",
    "        'Beige_Book': beige_book,\n",
    "        'Document_Label': document_label\n",
    "    })\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Processed Beige Book: {beige_book}\")\n",
    "\n",
    "# Create DataFrame with document-level results\n",
    "dfOpenAI_labels = pd.DataFrame(document_results)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "dfOpenAI_labels.to_csv('dfOpenAI_labels.csv', index=False)\n",
    "\n",
    "print(\"All documents processed and saved to 'dfOpenAI_labels.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>file_names</th>\n",
       "      <th>human_sentiment</th>\n",
       "      <th>scorer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1970_at (7)_chunk_1.txt</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>CS</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1970_bo (4)_chunk_2.txt</td>\n",
       "      <td>0.2</td>\n",
       "      <td>CS</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1970_ch (1)_chunk_4.txt</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>CS</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1970_ch (5)_chunk_2.txt</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>CS</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1970_ch (7)_chunk_2.txt</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>CS</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document               file_names  human_sentiment scorer     label\n",
       "0         1  1970_at (7)_chunk_1.txt             -0.9     CS  negative\n",
       "1         2  1970_bo (4)_chunk_2.txt              0.2     CS     mixed\n",
       "2         3  1970_ch (1)_chunk_4.txt             -0.5     CS  negative\n",
       "3         4  1970_ch (5)_chunk_2.txt             -0.7     CS  negative\n",
       "4         5  1970_ch (7)_chunk_2.txt             -0.5     CS  negative"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sentiment scores CSV\n",
    "excel_path = r\"C:/Users/MCOB PHD 14/Dropbox/Charlie's Dissertation/Beige Books/manual_sentiment.csv\"\n",
    "human_df = pd.read_csv(excel_path)\n",
    "\n",
    "# Define the label function\n",
    "def label_sentiment(score):\n",
    "    if score <= -0.3:\n",
    "        return 0  # Negative\n",
    "    elif score <= 0.2:\n",
    "        return 1  # Mixed\n",
    "    else:\n",
    "        return 2  # Positive\n",
    "\n",
    "# Apply the label function to the sentiment scores\n",
    "human_df['label'] = human_df['human_sentiment'].apply(label_sentiment)\n",
    "\n",
    "# convert label numbers to be negative, mixed, or positive\n",
    "human_df['label'] = human_df['label'].replace({0: 'negative', 1: 'mixed', 2: 'positive'})\n",
    "\n",
    "human_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beige_Book</th>\n",
       "      <th>Document_Label</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970_ch (1)_chunk_4.txt</td>\n",
       "      <td>ChatCompletionMessage(content='{\"label\": \"nega...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970_cl (6)_chunk_1.txt</td>\n",
       "      <td>ChatCompletionMessage(content='{\"label\":\"mixed...</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970_ny (1)_chunk_3.txt</td>\n",
       "      <td>ChatCompletionMessage(content='{\"label\":\"negat...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1971_kc (12)_chunk_2.txt</td>\n",
       "      <td>ChatCompletionMessage(content='{\"label\": \"mixe...</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1972_at (9)_chunk_2.txt</td>\n",
       "      <td>ChatCompletionMessage(content='{\"label\": \"mixe...</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Beige_Book  \\\n",
       "0   1970_ch (1)_chunk_4.txt   \n",
       "1   1970_cl (6)_chunk_1.txt   \n",
       "2   1970_ny (1)_chunk_3.txt   \n",
       "3  1971_kc (12)_chunk_2.txt   \n",
       "4   1972_at (9)_chunk_2.txt   \n",
       "\n",
       "                                      Document_Label Sentiment_Label  \n",
       "0  ChatCompletionMessage(content='{\"label\": \"nega...        negative  \n",
       "1  ChatCompletionMessage(content='{\"label\":\"mixed...           mixed  \n",
       "2  ChatCompletionMessage(content='{\"label\":\"negat...        negative  \n",
       "3  ChatCompletionMessage(content='{\"label\": \"mixe...           mixed  \n",
       "4  ChatCompletionMessage(content='{\"label\": \"mixe...           mixed  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ran in 1 minutes and 34.8 seconds\n",
    "    # The cost was $0.15\n",
    "\n",
    "import json\n",
    "\n",
    "# Function to extract label from the ChatCompletionMessage content\n",
    "def extract_label(chat_message):\n",
    "    try:\n",
    "        # Extract the content attribute directly\n",
    "        content = chat_message.content\n",
    "        # Parse the JSON string\n",
    "        content_dict = json.loads(content)\n",
    "        # Return the label value\n",
    "        return content_dict['label']\n",
    "    except (json.JSONDecodeError, KeyError, AttributeError) as e:\n",
    "        # Handle potential errors and return None or a default value\n",
    "        print(f\"Error extracting label: {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply the function to the Document_Label column to create a new column\n",
    "dfOpenAI_labels['Sentiment_Label'] = dfOpenAI_labels['Document_Label'].apply(extract_label)\n",
    "\n",
    "# Need to join dfOpenAI_labels to human_df keeping only the rows that are in both\n",
    "\n",
    "# Merge the two DataFrames on the 'Beige_Book' column in dfOpenAI_labels and 'file_names' column in human_scores_df\n",
    "# Just keep the label column from human_df and the Sentiment_Label column from dfOpenAI_labels\n",
    "merged_df = pd.merge(dfOpenAI_labels, human_df[['file_names', 'label']], left_on='Beige_Book', right_on='file_names', how='inner')\n",
    "\n",
    "# Rename the 'label' column to 'Human_Label'\n",
    "merged_df.rename(columns={'label': 'Human_Label'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "#merged_df.to_csv('GPT_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.96      0.31      0.47        78\n",
      "    negative       0.86      0.62      0.72        39\n",
      "       mixed       0.53      0.94      0.68        83\n",
      "\n",
      "    accuracy                           0.63       200\n",
      "   macro avg       0.78      0.62      0.62       200\n",
      "weighted avg       0.76      0.63      0.60       200\n",
      "\n",
      "Accuracy: 0.630\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(merged_df['Human_Label'], merged_df['Sentiment_Label'], labels=['positive', 'negative', 'mixed'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(merged_df['Human_Label'], merged_df['Sentiment_Label'])\n",
    "print(f'Accuracy: {accuracy:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
