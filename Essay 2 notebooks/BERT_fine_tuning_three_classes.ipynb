{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Document                file_names  human_sentiment scorer  label  \\\n",
      "0          1   1970_at (7)_chunk_1.txt             -0.9     CS      0   \n",
      "1          2   1970_bo (4)_chunk_2.txt              0.2     CS      1   \n",
      "2          3   1970_ch (1)_chunk_4.txt             -0.5     CS      0   \n",
      "3          4   1970_ch (5)_chunk_2.txt             -0.7     CS      0   \n",
      "4          5   1970_ch (7)_chunk_2.txt             -0.5     CS      0   \n",
      "5          6   1970_cl (6)_chunk_1.txt              0.3     CS      2   \n",
      "6          7   1970_da (2)_chunk_4.txt              0.4     CS      2   \n",
      "7          8   1970_kc (3)_chunk_1.txt             -0.8     CS      0   \n",
      "8          9   1970_kc (5)_chunk_3.txt             -0.5     CS      0   \n",
      "9         10   1970_mn (3)_chunk_2.txt              0.4     CS      2   \n",
      "10        11   1970_ns (1)_chunk_2.txt             -0.8     CS      0   \n",
      "11        12   1970_ns (2)_chunk_3.txt             -0.6     CS      0   \n",
      "12        13   1970_ny (1)_chunk_3.txt             -0.2     CS      1   \n",
      "13        14  1971_at (12)_chunk_1.txt             -0.7     CS      0   \n",
      "14        15   1971_at (8)_chunk_3.txt              0.2     CS      1   \n",
      "15        16   1971_at (9)_chunk_3.txt              0.0     CS      1   \n",
      "16        17   1971_bo (2)_chunk_1.txt              0.1     CS      1   \n",
      "17        18   1971_cl (4)_chunk_4.txt             -0.1     CS      1   \n",
      "18        19   1971_da (2)_chunk_2.txt              0.1     CS      1   \n",
      "19        20   1971_da (4)_chunk_3.txt             -0.1     CS      1   \n",
      "20        21  1971_kc (12)_chunk_2.txt              0.1     CS      1   \n",
      "21        22   1971_mn (1)_chunk_1.txt              0.4     CS      2   \n",
      "22        23   1971_mn (9)_chunk_2.txt              0.8     CS      2   \n",
      "23        24   1971_ns (1)_chunk_2.txt              0.1     CS      1   \n",
      "24        25  1971_ns (11)_chunk_2.txt             -0.7     CS      0   \n",
      "25        26  1971_ns (12)_chunk_2.txt             -0.3     CS      0   \n",
      "26        27   1971_ns (3)_chunk_3.txt             -0.2     CS      1   \n",
      "27        28   1971_ny (5)_chunk_1.txt             -0.6     CS      0   \n",
      "28        29   1971_ny (6)_chunk_1.txt             -0.6     CS      0   \n",
      "29        30   1971_ny (9)_chunk_1.txt             -0.5     CS      0   \n",
      "30        31   1971_ri (1)_chunk_1.txt              0.7     CS      2   \n",
      "31        32   1971_ri (1)_chunk_3.txt              0.9     CS      2   \n",
      "32        33  1971_ri (11)_chunk_3.txt              0.1     CS      1   \n",
      "33        34  1971_ri (12)_chunk_2.txt             -0.2     CS      1   \n",
      "34        35   1972_at (9)_chunk_2.txt              0.6     CS      2   \n",
      "35        36   1972_ch (4)_chunk_2.txt              0.1     CS      1   \n",
      "36        37  1972_da (11)_chunk_3.txt              0.7     CS      2   \n",
      "37        38   1972_da (3)_chunk_1.txt              0.5     CS      2   \n",
      "38        39  1972_kc (11)_chunk_1.txt              0.3     CS      2   \n",
      "39        40   1972_kc (5)_chunk_1.txt              0.9     CS      2   \n",
      "40        41   1972_ns (9)_chunk_4.txt              0.5     CS      2   \n",
      "41        42  1972_ny (10)_chunk_2.txt              0.8     CS      2   \n",
      "42        43   1972_ny (4)_chunk_2.txt              0.1     CS      1   \n",
      "43        44   1972_ri (1)_chunk_2.txt              0.9     CS      2   \n",
      "44        45   1972_sf (5)_chunk_3.txt              0.7     CS      2   \n",
      "45        46   1972_sf (6)_chunk_2.txt              0.3     CS      2   \n",
      "46        47   1973_at (8)_chunk_1.txt              0.9     CS      2   \n",
      "47        48  1973_bo (10)_chunk_3.txt             -0.2     CS      1   \n",
      "48        49   1973_ch (2)_chunk_2.txt              0.0     CS      1   \n",
      "49        50   1973_ch (3)_chunk_4.txt              0.2     CS      1   \n",
      "\n",
      "                                                 text  \n",
      "0   June 17 , 1970 Summary of Findings Sixth Distr...  \n",
      "1   of jewelry manufacturers in southern Massachus...  \n",
      "2   rates and terms have not eased much . There ar...  \n",
      "3   over the slowness of payments on receivables b...  \n",
      "4   outside Chicago , department stores report int...  \n",
      "5   July 15 , 1970 Economic activity in the Fourth...  \n",
      "6   noted-customers are very aware of price compet...  \n",
      "7   October 14 , 1970 There is evidence that futur...  \n",
      "8   be delivered on the east coast at substantiall...  \n",
      "9   this expectation primarily on the fact that ab...  \n",
      "10  physical volume of sales in large department s...  \n",
      "11  . Retailers ' inventories are generally being ...  \n",
      "12  wage settlements and over continued upward pre...  \n",
      "13  January 12 , 1971 A research department survey...  \n",
      "14  , and by that time market conditions may not p...  \n",
      "15  and professional economists in the Atlanta reg...  \n",
      "16  November 10 , 1971 Our directors indicate that...  \n",
      "17  the part of large companies because the orderi...  \n",
      "18  As for the present outlook for prices , about ...  \n",
      "19  anticipate that their export sales and foreign...  \n",
      "20  adversely affected by curtailment of Governmen...  \n",
      "21  December 8 , 1971 Uncertainty over Phase II ha...  \n",
      "22  was in existing housing and new construction h...  \n",
      "23  found mortgage money abundant , and rates and ...  \n",
      "24  in the former case , the increase is a recover...  \n",
      "25  higher sales volume is extending into the new ...  \n",
      "26  in conjunction with liberalized depreciation a...  \n",
      "27  August 18 , 1971 The pessimism regarding the e...  \n",
      "28  July 21 , 1971 The opinions expressed recently...  \n",
      "29  April 6 , 1971 The opinions expressed by the D...  \n",
      "30  December 8 , 1971 Results of our regular surve...  \n",
      "31  3 per cent gain in receipts from crops . Quali...  \n",
      "32  the increase in the District since last August...  \n",
      "33  sales . Manufacturers report further slight in...  \n",
      "34  build offshore nuclear generators . Two wareho...  \n",
      "35  ) written justifications , and ( 3 ) more freq...  \n",
      "36  indicators of retail spending were mixed . Reg...  \n",
      "37  October 11 , 1972 Indicators of economic activ...  \n",
      "38  February 9 , 1972 Phase II policies , and comp...  \n",
      "39  August 9 , 1972 The vigor in the economic reco...  \n",
      "40  in the Boston area convey a growing disillusio...  \n",
      "41  high-quality New York City department store wi...  \n",
      "42  commitments for `` pay out '' in the first hal...  \n",
      "43  a shortage of both skilled and unskilled labor...  \n",
      "44  expanding inventories throughout the manufactu...  \n",
      "45  advantage of domestic car dealers in having be...  \n",
      "46  May 9 , 1973 General economic activity is expa...  \n",
      "47  wage increases for fear of bringing back Phase...  \n",
      "48  . There is great uneasiness that layoffs will ...  \n",
      "49  the base of Lake Michigan . The Chrysler labor...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the sentiment scores\n",
    "excel_path = r\"C:/Users/MCOB PHD 14/Dropbox/Charlie's Dissertation/Beige Books/manual_sentiment.csv\"\n",
    "sentiment_data = pd.read_csv(excel_path)\n",
    "\n",
    "# create labels\n",
    "    # -.2 to -2: Mixed\n",
    "    # <-.2: Negative\n",
    "    # >.2: Positive\n",
    "    \n",
    "\n",
    "def label_sentiment(score):\n",
    "    if score <= -0.3:\n",
    "        return 0\n",
    "    elif score <= 0.2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2  \n",
    "    \n",
    "sentiment_data['label'] = sentiment_data['human_sentiment'].apply(label_sentiment)\n",
    "    \n",
    "# Define path where text files are stored\n",
    "text_files_dir = r\"C:/Users/MCOB PHD 14/Dropbox/Charlie's Dissertation/Beige Books/selected_chunks2\"\n",
    "\n",
    "# Load the tokenized text files\n",
    "text_data = {}\n",
    "for filename in os.listdir(text_files_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(text_files_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text_data[filename] = file.read()\n",
    "\n",
    "# Combine the two\n",
    "# Create a DataFrame from text_data\n",
    "text_df = pd.DataFrame(list(text_data.items()), columns=['file_names', 'text'])\n",
    "\n",
    "# Join the sentiment data with the text data based on 'file_names'\n",
    "combined_data = pd.merge(sentiment_data, text_df, on='file_names')\n",
    "\n",
    "# Preview the combined data\n",
    "print(combined_data.head(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MCOB PHD 14\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 6713, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2254, 2654,  ...,    0,    0,    0],\n",
      "        [ 101, 1011, 1011,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2096, 1996,  ...,    0,    0,    0],\n",
      "        [ 101, 2152, 1011,  ...,    0,    0,    0],\n",
      "        [ 101, 2195, 5501,  ...,    0,    0,    0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([2, 2, 2, 2, 1, 1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text data from the 'text' column in combined_data\n",
    "def tokenize_texts(texts, max_length=512):\n",
    "    tokenized_texts = []\n",
    "    for text in texts:\n",
    "        tokenized_input = tokenizer(\n",
    "            text, \n",
    "            padding='max_length',  # Pad to max_length\n",
    "            truncation=True,       # Truncate if longer than max_length\n",
    "            max_length=max_length, # Maximum length for input\n",
    "            return_tensors='pt'    # Return PyTorch tensors\n",
    "        )\n",
    "        tokenized_texts.append({\n",
    "            'input_ids': tokenized_input['input_ids'].squeeze(), \n",
    "            'attention_mask': tokenized_input['attention_mask'].squeeze()\n",
    "        })\n",
    "    return tokenized_texts\n",
    "\n",
    "# Tokenize the texts from the 'text' column in combined_data\n",
    "texts = combined_data['text'].tolist()  # Extract the texts\n",
    "tokenized_texts = tokenize_texts(texts)\n",
    "\n",
    "# Extract the labels from the 'label' column in combined_data\n",
    "labels = combined_data['label'].tolist()\n",
    "\n",
    "# Create a custom Dataset class using tokenized texts and labels\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenized_texts[idx]['input_ids']\n",
    "        attention_mask = self.tokenized_texts[idx]['attention_mask']\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': label}\n",
    "\n",
    "# Create the dataset using the tokenized texts and labels\n",
    "dataset = SentimentDataset(tokenized_texts, labels)\n",
    "\n",
    "# Create a DataLoader to batch the data\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Example: Iterate over the DataLoader and print a batch\n",
    "for batch in dataloader:\n",
    "    print(batch['input_ids'])\n",
    "    print(batch['attention_mask'])\n",
    "    print(batch['labels'])\n",
    "    break  # Remove break to iterate through the entire dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning model\n",
    "\n",
    "Warning: This takes about 70 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 375/375 [1:12:54<00:00, 11.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4374.405, 'train_samples_per_second': 0.686, 'train_steps_per_second': 0.086, 'train_loss': 1.0100785319010417, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=1.0100785319010417, metrics={'train_runtime': 4374.405, 'train_samples_per_second': 0.686, 'train_steps_per_second': 0.086, 'total_flos': 789354427392000.0, 'train_loss': 1.0100785319010417, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification\n",
    "\n",
    "# Load the BERT model with a classification head\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "# Define training arguments (I just used all the default values)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    warmup_steps=500,                # number of warmup steps\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    eval_strategy=\"no\",              # No evaluation during training\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=dataset,          # Use the full dataset for training\n",
    ")\n",
    "\n",
    "# Fine-tune the model on the full dataset\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./BeigeBERT_three\\\\tokenizer_config.json',\n",
       " './BeigeBERT_three\\\\special_tokens_map.json',\n",
       " './BeigeBERT_three\\\\vocab.txt',\n",
       " './BeigeBERT_three\\\\added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('./BeigeBERT_three')\n",
    "tokenizer.save_pretrained('./BeigeBERT_three')\n",
    "\n",
    "# The model and tokenizer can be loaded later for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess performance\n",
    "\n",
    "Note I have not split into testing/training\n",
    "i.e. this data is being tested on the same data it was trained on\n",
    "\n",
    "Results are promising, showing good balance between classes and consistent performance across classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [06:59<00:00,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.74      0.87      0.80       198\n",
      "     Neutral       0.81      0.79      0.80       413\n",
      "    Positive       0.91      0.86      0.89       389\n",
      "\n",
      "    accuracy                           0.83      1000\n",
      "   macro avg       0.82      0.84      0.83      1000\n",
      "weighted avg       0.84      0.83      0.83      1000\n",
      "\n",
      "Accuracy on training data: 0.8320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Performance assessment\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions from the model\n",
    "# The predict function returns predictions, actual labels, and other info\n",
    "predictions, true_labels, _ = trainer.predict(dataset)\n",
    "\n",
    "# Convert predictions to label indices\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print a classification report\n",
    "report = classification_report(true_labels, predicted_labels, target_names=[\"Negative\", \"Mixed\", \"Positive\"])\n",
    "print(report)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f'Accuracy is: {accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
