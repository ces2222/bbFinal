{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file sets up the datasets. It can take some time to run.\n",
    "\n",
    "Subsequent analyses can just import the CSV files it creates:\n",
    "\n",
    "1. bbNoText.csv is all of the numerical values, plus the dates. It doesn't include the Beige Books texts. It's a lot smaller file.\n",
    "2. bbText.csv includes all of the Beige Book texts and the sentiment scores per sentence. It's a larger file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas for numerical variable analysis and os, numpy, string, and nltk for text variable analysis.\n",
    "\n",
    "import pandas as pd\n",
    "import os as os\n",
    "import numpy as np\n",
    "import string as string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "districtLevels = pd.read_csv('district_coincident_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting date column to date-time format\n",
    "districtLevels['Date'] = pd.to_datetime(districtLevels['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape dataframe from wide to long\n",
    "    # This does it for both the levels and differences from national average\n",
    "    # Having decided only to use the levels, I have commented out the diffs\n",
    "districtLongL = pd.melt(districtLevels, id_vars=[\"Date\"], var_name=\"District\", value_name=\"Value\")\n",
    "#districtLong = pd.melt(districtDiffs, id_vars=[\"Date\", \"USPHCI\"], var_name=\"District\", value_name=\"Value\")\n",
    "\n",
    "# Extract the numeric part from the \"District\" names and convert to integers\n",
    "districtLongL[\"District\"] = districtLongL[\"District\"].str.extract(r'(\\d+)').astype(int)\n",
    "#districtLong[\"District\"] = districtLong[\"District\"].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "# Sort the DataFrame by \"Date\" and \"District\" columns\n",
    "districtLongL = districtLongL.sort_values(by=[\"Date\", \"District\"])\n",
    "#districtLong = districtLong.sort_values(by=[\"Date\", \"District\"])\n",
    "\n",
    "# Reset the index\n",
    "districtLongL.reset_index(drop=True, inplace=True)\n",
    "#districtLong.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping of city abbreviations to district numbers\n",
    "city_to_district = {\n",
    "    \"bo\": 1,\n",
    "    \"ny\": 2,\n",
    "    \"ph\": 3,\n",
    "    \"cl\": 4,\n",
    "    \"ri\": 5,\n",
    "    \"at\": 6,\n",
    "    \"ch\": 7,\n",
    "    \"sl\": 8,\n",
    "    \"mn\": 9,\n",
    "    \"kc\": 10,\n",
    "    \"da\": 11,\n",
    "    \"sf\": 12,\n",
    "}\n",
    "\n",
    "# Define a mapping of month codes (as named in the original text files of the Beige Books) to month names\n",
    "month_to_name = {\n",
    "    \"1\": \"December\",\n",
    "    \"2\": \"October\",\n",
    "    \"3\": \"September\",\n",
    "    \"4\": \"July\",\n",
    "    \"5\": \"June\",\n",
    "    \"6\": \"May\",\n",
    "    \"7\": \"March\",\n",
    "    \"8\": \"January\",\n",
    "}\n",
    "\n",
    "# Define a mapping of month names to month numbers\n",
    "name_to_month_numerical = {\n",
    "    \"December\": \"12\",\n",
    "    \"October\": \"10\",\n",
    "    \"September\": \"09\",\n",
    "    \"July\": \"07\",\n",
    "    \"June\": \"06\",\n",
    "    \"May\": \"05\",\n",
    "    \"March\": \"03\",\n",
    "    \"January\": \"01\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the Beige Books texts as a dataframe\n",
    "    # Also including the filename so we can know which region, month, and year it's from\n",
    "\n",
    "# Setting the directory where the text files of the Beige Books are stored\n",
    "directory_path = \"C:/Users/MCOB PHD 14/Dropbox/Charlie's Dissertation/Beige Books\"\n",
    "\n",
    "# Create empty lists to store text and file names\n",
    "textDF = []\n",
    "file_names = []\n",
    "\n",
    "# Loop through the files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "            # Append the text to the text_data list\n",
    "            textDF.append(file.read())\n",
    "            # Append the file name to the file_names list\n",
    "            file_names.append(filename)\n",
    "\n",
    "# Create a DataFrame from the text and file_names lists\n",
    "data = {'text': textDF, 'file_name': file_names}\n",
    "BeigeBookDF = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the nltk vader package for sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# Initialize the sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function and run it on the Beige Book dataframe\n",
    "def analyze_sentiment(BeigeBookDF):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(BeigeBookDF)\n",
    "    \n",
    "    compound_score = sentiment_scores['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        sentiment = 'positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "    \n",
    "    return compound_score, sentiment\n",
    "\n",
    "BeigeBookDF['compound_score'], BeigeBookDF['sentiment'] = zip(*BeigeBookDF['text'].apply(analyze_sentiment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that is a copy of BeigeBookDF\n",
    "bbDF = BeigeBookDF\n",
    "\n",
    "# Extract the year, city, and month code into separate columns\n",
    "bbDF[['Year', 'City', 'MonthCode']] = BeigeBookDF['file_name'].str.extract(r'(\\d{4})_(\\w{2}) \\((\\d)\\).txt')\n",
    "\n",
    "# Map the city abbreviations to district numbers\n",
    "bbDF['District'] = bbDF['City'].map(city_to_district)\n",
    "\n",
    "# Map the month code to month names\n",
    "bbDF['MonthName'] = bbDF['MonthCode'].map(month_to_name)\n",
    "\n",
    "# Map the month names to numerical month numbers\n",
    "bbDF['MonthNumerical'] = bbDF['MonthName'].map(name_to_month_numerical)\n",
    "\n",
    "# Create a new column with the formatted date\n",
    "bbDF['Date'] = bbDF['Year'] + '-' + BeigeBookDF['MonthNumerical'] + '-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to a datetime data type\n",
    "bbDF['Date'] = pd.to_datetime(bbDF['Date'])\n",
    "\n",
    "# Use the merge function to join the DataFrames based on the 'Date' and 'District' columns\n",
    "bbMerged = districtLongL.merge(bbDF, on=['Date', 'District'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the Value column to 'econ_index'\n",
    "bbMerged = bbMerged.rename(columns={'Value': 'econ_index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean sentence per Beige Book compound score\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_sentiment_scores(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    scores = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentiment_score = sia.polarity_scores(sentence)\n",
    "        scores.append(sentiment_score)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2484: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory_path, filename)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 7\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      9\u001b[0m sentiment_scores \u001b[38;5;241m=\u001b[39m calculate_sentiment_scores(text)\n\u001b[0;32m     10\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m'\u001b[39m: filename,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment Scores\u001b[39m\u001b[38;5;124m'\u001b[39m: sentiment_scores\n\u001b[0;32m     13\u001b[0m })\n",
      "File \u001b[1;32mc:\\Users\\MCOB PHD 14\\anaconda3\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2484: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".txt\"):  \n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        sentiment_scores = calculate_sentiment_scores(text)\n",
    "        results.append({\n",
    "            'File': filename,\n",
    "            'Sentiment Scores': sentiment_scores\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to do the above for every document\n",
    "    # Going to start with the bbMerged file, which already has Beige Book documents listed by city and date\n",
    "    # Going to create a new df that drops some of the extra stuff leftover in bbMerged\n",
    "\n",
    "bbSimple = bbMerged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculating the compound score for each sentence in each Beige Book\n",
    "    # Then saving that as a list within the bbSimple dataframe\n",
    "\n",
    "# Initialize the SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to calculate the compound sentiment score\n",
    "def calculate_sentiment(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    compound_scores = [sid.polarity_scores(sentence)['compound'] for sentence in sentences]\n",
    "    return compound_scores\n",
    "\n",
    "# Assuming 'bbSimple' is your DataFrame\n",
    "bbSimple['compound_sentiment'] = bbSimple['text'].apply(calculate_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculate the mean and median sentiment scores for each document\n",
    "# bbSimple['mean_sentiment'] = bbSimple['compound_sentiment'].apply(lambda x: sum(x) / len(x) if len(x) > 0 else 0)\n",
    "# bbSimple['median_sentiment'] = bbSimple['compound_sentiment'].apply(lambda x: sorted(x)[len(x) // 2] if len(x) > 0 else 0)\n",
    "\n",
    "bbSimple['mean_sentiment'] = bbSimple['compound_sentiment'].apply(lambda x: sum(x) / len(x) if len(x) > 0 else 0)\n",
    "bbSimple['median_sentiment'] = bbSimple['compound_sentiment'].apply(lambda x: sorted(x)[len(x) // 2] if len(x) > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing quartiles as well\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# DbbSimple[['econ_index', 'compound_sentiment', 'mean_sentiment', 'IQR_sentiment']].median()efine a list of desired quartiles (25th, 50th, and 75th percentiles)\n",
    "quartiles = [25, 50, 75]\n",
    "\n",
    "# Calculate the specified quartiles for each document\n",
    "for quartile in quartiles:\n",
    "    column_name = f'{quartile}th_quartile_sentiment'\n",
    "    bbSimple[column_name] = bbSimple['compound_sentiment'].apply(lambda x: np.percentile(x, quartile) if len(x) > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating interquartile range\n",
    "bbSimple['IQR_sentiment'] = bbSimple['75th_quartile_sentiment'] - bbSimple['25th_quartile_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "path = \"C:/Users/MCOB PHD 14/Desktop/bbFinal/Data/trigrams.csv\"\n",
    "trigrams = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLTK's sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to calculate sentiment for a given text\n",
    "def calculate_sentiment(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    compound_score = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        scores = sia.polarity_scores(sentence)\n",
    "        compound_score += scores['compound']\n",
    "\n",
    "    average_compound_score = compound_score / len(sentences)\n",
    "    return average_compound_score\n",
    "\n",
    "# List to store trigram and sentiment information\n",
    "trigram_sentiment_list = []\n",
    "\n",
    "\n",
    "# Loop through each trigram\n",
    "for _, trigram_row in trigrams.iterrows():\n",
    "    trigram = trigram_row['trigram']\n",
    "\n",
    "    # Loop through each document in the subset\n",
    "    for _, doc_row in bbSimple.iterrows():\n",
    "        document = doc_row['text']\n",
    "        date = doc_row['Date']\n",
    "        district = doc_row['District']\n",
    "        year = doc_row['Year']\n",
    "        city = doc_row['City']\n",
    "        month = doc_row['MonthName']\n",
    "\n",
    "        # Tokenize the document into sentences\n",
    "        sentences = sent_tokenize(document)\n",
    "\n",
    "        # Check if the trigram is present in any sentence\n",
    "        for sentence_index, sentence in enumerate(sentences):\n",
    "            if trigram in sentence:\n",
    "                # Extract the surrounding two sentences\n",
    "                trigram_index = sentence_index\n",
    "                start_index = max(0, trigram_index - 1)\n",
    "                end_index = min(len(sentences), trigram_index + 2)\n",
    "                context_sentences = sentences[start_index:end_index]\n",
    "\n",
    "                # Join the sentences to form a paragraph\n",
    "                context_paragraph = ' '.join(context_sentences)\n",
    "\n",
    "                # Calculate sentiment for the extracted context\n",
    "                sentiment_score = calculate_sentiment(context_paragraph)\n",
    "\n",
    "                # Append trigram, sentiment, and context to the list\n",
    "                trigram_sentiment_list.append({\n",
    "                    'Trigram': trigram,\n",
    "                    'Sentiment': sentiment_score,\n",
    "                    'Context': context_paragraph,\n",
    "                    'Document': document,\n",
    "                    'Date': date,\n",
    "                    'District': district,\n",
    "                    'Year': year,\n",
    "                    'City': city,\n",
    "                    'Month': month\n",
    "                })\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "result_df3 = pd.DataFrame(trigram_sentiment_list)\n",
    "\n",
    "# Export the DataFrame to a CSV file in the default working directory\n",
    "#result_df3.to_csv('trigram_sentiment_results.csv', index=False)\n",
    "\n",
    "# Display a message indicating the export is successful\n",
    "#print('DataFrame successfully exported to trigram_sentiment_results.csv in the default working directory.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of each trigram's sentiment + all other trigrams' sentiment found in a document\n",
    "    # Some documents don't have any Trigrams; some have multiple\n",
    "mean_sentiments = result_df3.groupby(['Document'])['Sentiment'].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_sentiments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mean_sentiments\u001b[38;5;241m.\u001b[39mcorr()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_sentiments' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'Sentiment' column to 'Sentiment_Trigram'\n",
    "mean_sentiments = mean_sentiments.rename(columns={'Sentiment': 'sentiment_trigram'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge mean_sentiments back to bbSimple based on the common columns 'text' and 'Document'\n",
    "bbTri = pd.merge(bbSimple, mean_sentiments, left_on='text', right_on='Document', how='left')\n",
    "\n",
    "# Drop the duplicate 'Document' column, which is now redundant\n",
    "bbTri = bbTri.drop('Document', axis=1)\n",
    "\n",
    "# Set N/As to equal zero\n",
    "bbTri['sentiment_trigram'] = bbTri['sentiment_trigram'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to create a DV that's percent change in econ_index_levels\n",
    "\n",
    "bbTri.sort_values(by=['District', 'Date'], inplace=True)\n",
    "bbTri['econ_index_change'] = bbTri.groupby('District')['econ_index'].pct_change() *100\n",
    "\n",
    "# Now want to create a new dummy that equals 1 if the change is positive\n",
    "bbTri['econ_index_change_dummy'] = (bbTri['econ_index_change'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbNoText = bbTri[['file_name', 'Date', 'Year', 'District', 'econ_index', 'econ_index_change', 'econ_index_change_dummy', 'compound_score', 'mean_sentiment', 'median_sentiment', 'IQR_sentiment', 'sentiment_trigram']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file in the default working directory\n",
    "bbNoText.to_csv('bbNoText.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export bbTri, which has the full text included and sentence level compound scores (calculated with VADER)\n",
    "bbTri.to_csv('bbText.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
